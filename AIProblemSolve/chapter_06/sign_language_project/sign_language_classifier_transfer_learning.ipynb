{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning for Vision Systems](https://www.manning.com/books/deep-learning-for-vision-systems?a_aid=compvisionbookcom&a_bid=90abff15) Book\n",
    "\n",
    "\n",
    "## Chapter 6 Project: Sign language exercise\n",
    "\n",
    "---\n",
    "### 1. Import the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'dataset/train'\n",
    "valid_path  = 'dataset/valid'\n",
    "test_path  = 'dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VGG16 base model pre-trained on ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 20:45:17.521593: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. freeze the classification layers in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 7,084,554\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use “get_layer” method to save the last layer of the network\n",
    "last_layer = base_model.get_layer('global_average_pooling2d')\n",
    "\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/7r4z_h8s5jq0_2bm_t7_4zx40000gn/T/ipykernel_95531/4004320113.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 46s 3s/step - loss: 3.3220 - accuracy: 0.2833 - val_loss: 1.9051 - val_accuracy: 0.3556\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 1.4826 - accuracy: 0.5000 - val_loss: 1.2463 - val_accuracy: 0.6444\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 50s 3s/step - loss: 1.2214 - accuracy: 0.5833 - val_loss: 0.9751 - val_accuracy: 0.7222\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 44s 2s/step - loss: 0.7056 - accuracy: 0.7500 - val_loss: 0.6404 - val_accuracy: 0.7778\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 44s 2s/step - loss: 0.4285 - accuracy: 0.8444 - val_loss: 0.2572 - val_accuracy: 0.9444\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 44s 2s/step - loss: 0.3068 - accuracy: 0.9000 - val_loss: 0.3387 - val_accuracy: 0.9444\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 44s 2s/step - loss: 0.2812 - accuracy: 0.8944 - val_loss: 0.2081 - val_accuracy: 0.9111\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.2701 - accuracy: 0.9222 - val_loss: 0.0986 - val_accuracy: 0.9778\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 47s 3s/step - loss: 0.2098 - accuracy: 0.9167 - val_loss: 0.2368 - val_accuracy: 0.9556\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 48s 3s/step - loss: 0.1360 - accuracy: 0.9556 - val_loss: 0.2599 - val_accuracy: 0.9556\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 45s 3s/step - loss: 0.1001 - accuracy: 0.9611 - val_loss: 0.1077 - val_accuracy: 0.9778\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.0904 - accuracy: 0.9722 - val_loss: 0.1700 - val_accuracy: 0.9556\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.0414 - accuracy: 0.9833 - val_loss: 0.1781 - val_accuracy: 0.9333\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9778\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 45s 3s/step - loss: 0.0269 - accuracy: 0.9944 - val_loss: 0.3014 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 45s 3s/step - loss: 0.0951 - accuracy: 0.9722 - val_loss: 0.0488 - val_accuracy: 0.9889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 43s 2s/step - loss: 0.0709 - accuracy: 0.9778 - val_loss: 0.2218 - val_accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 46s 3s/step - loss: 0.0485 - accuracy: 0.9778 - val_loss: 0.0473 - val_accuracy: 0.9889\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 45s 3s/step - loss: 0.0318 - accuracy: 0.9942 - val_loss: 0.0459 - val_accuracy: 0.9889\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 44s 2s/step - loss: 0.0420 - accuracy: 0.9944 - val_loss: 0.2859 - val_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. create the confusion matrix to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 735.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('signlanguage.model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 2s/step - loss: 0.1421 - accuracy: 0.9600\n",
      "\n",
      "Testing loss: 0.1421\n",
      "Testing accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "cm = confusion_matrix(np.argmax(test_targets, axis=1),\n",
    "                      np.argmax(new_model.predict(test_tensors), axis=1))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "indexes = np.arange(len(cm_labels))\n",
    "for i in indexes:\n",
    "    for j in indexes:\n",
    "        plt.text(j, i, cm[i, j])\n",
    "plt.xticks(indexes, cm_labels, rotation=90)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.yticks(indexes, cm_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
