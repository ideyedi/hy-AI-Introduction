{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **AAI0026 Practice 3: GraphSage and GAT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Practice 2, we constructed GNN models by using PyTorch Geometric built in GCN layer, the `GCNConv`. In this Colab we will implement the **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) and **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)) layers directly. Then we will run our models on the CORA dataset, which is a standard citation network benchmark dataset.\n",
        "\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_m9l6OYCQZP",
        "outputId": "d6856411-9db8-4008-f361-fec253b92176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 15.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 467 kB 14.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PRfgbfTjCRD_",
        "outputId": "fef949d7-b0e8-4f05-e8fe-d62b2c581375"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoXlf4MtYrbz"
      },
      "source": [
        "# 1 GNN Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQy2RBfgYut4"
      },
      "source": [
        "## Implementing Layer Modules\n",
        "\n",
        "In colab 2, we implemented a network using GCN in node and graph classification tasks. However, the GCN module we used in colab 2 is from the official library. For this problem, we will provide you with a general Graph Neural Network Stack, where you'll be able to plugin your own modules of GraphSAGE and GATs. We will use our implementations to complete node classification on CORA, which is a standard citation network benchmark dataset. In this dataset, nodes correspond to documents and edges correspond to undirected citations. Each node has a class label. The node features are elements of a bag-or-words representation of a document. For the Cora dataset, there are 2708 nodes, 5429 edges, 7 prediction classes for nodes, and 1433 features per node. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ne6Gw-CT5G"
      },
      "source": [
        "## GNN Stack Module\n",
        "\n",
        "Below is the implementation for a general GNN Module that could plugin any layers, including **GraphSage**, **GAT**, etc. This module is provided for you, and your own **GraphSage** and **GAT** layers will function as components in the GNNStack Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ys8vZAFPCWWe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
        "                                    OptTensor)\n",
        "\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "# https://greeksharifa.github.io/pytorch/2021/09/04/MP/\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "        self.emb = emb\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GraphSage':\n",
        "            return GraphSage\n",
        "        elif model_type == 'GAT':\n",
        "            # When applying GAT with num heads > 1, one needs to modify the \n",
        "            # input and output dimension of the conv layers (self.convs),\n",
        "            # to ensure that the input dim of the next layer is num heads\n",
        "            # multiplied by the output dim of the previous layer.\n",
        "            # HINT: In case you want to play with multiheads, you need to change the for-loop when builds up self.convs to be\n",
        "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
        "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "          \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        if self.emb == True:\n",
        "            return x\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syDtxjxoCZgq"
      },
      "source": [
        "## GraphSage Implementation\n",
        "\n",
        "Now let's start working on our own implementation of layers! This part is to get you familiar with how to implement Pytorch layer based on Message Passing. You will be implementing the **forward**, **message** and **aggregate** functions.\n",
        "\n",
        "Generally, the **forward** function is where the actual message passing is conducted. All logic in each iteration happens in **forward**, where we'll call **propagate** function to propagate information from neighbor nodes to central nodes.  So the general paradigm will be pre-processing -> propagate -> post-processing.\n",
        "\n",
        "Recall the process of message passing we introduced. **propagate** further calls **message** which transforms information of neighbor nodes into messages, **aggregate** which aggregates all messages from neighbor nodes into one, and **update** which further generates the embedding for nodes in the next iteration.\n",
        "\n",
        "Our implementation is slightly variant from this, where we'll not explicitly implement **update**, but put the logic for updating nodes in **forward** function. To be more specific, after information is propagated, we can further conduct some operations on the output of **propagate**. The output of **forward** is exactly the embeddings after the current iteration.\n",
        "\n",
        "In addition, tensors passed to **propagate()** can be mapped to the respective nodes $i$ and $j$ by appending _i or _j to the variable name, .e.g. x_i and x_j. Note that we generally refer to $i$ as the central nodes that aggregates information, and refer to $j$ as the neighboring nodes, since this is the most common notation.\n",
        "\n",
        "Please find more details in the comments. One thing to note is that we're adding **skip connections** to our GraphSage. Formally, the update rule for our model is described as below:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
        "\\end{equation}\n",
        "\n",
        "For simplicity, we use mean aggregations where:\n",
        "\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
        "\\end{equation}\n",
        "\n",
        "Additionally, $\\ell$-2 normalization is applied after each iteration.\n",
        "\n",
        "In order to complete the work correctly, we have to understand how the different functions interact with each other. In **propagate** we can pass in any parameters we want. For example, we pass in $x$ as an parameter:\n",
        "\n",
        "... = propagate(..., $x$=($x_{central}$, $x_{neighbor}$), ...)\n",
        "\n",
        "Here $x_{central}$ and $x_{neighbor}$ represent the features from **central** nodes and from **neighbor** nodes. If we're using the same representations from central and neighbor, then $x_{central}$ and $x_{neighbor}$ could be identical.\n",
        "\n",
        "Suppose $x_{central}$ and $x_{neighbor}$ are both of shape N * d, where N is number of nodes, and d is dimension of features.\n",
        "\n",
        "Then in message function, we can take parameters called $x\\_i$ and $x\\_j$. Usually $x\\_i$ represents \"central nodes\", and $x\\_j$ represents \"neighbor nodes\". Pay attention to the shape here: $x\\_i$ and $x\\_j$ are both of shape E * d (**not N!**). $x\\_i$ is obtained by concatenating the embeddings of central nodes of all edges through lookups from $x_{central}$ we passed in propagate. Similarly, $x\\_j$ is obtained by concatenating the embeddings of neighbor nodes of all edges through lookups from $x_{neighbor}$ we passed in propagate.\n",
        "\n",
        "Let's look at an example. Suppose we have 4 nodes, so $x_{central}$ and $x_{neighbor}$ are of shape 4 * d. We have two edges (1, 2) and (3, 0). Thus, $x\\_i$ is obtained by $[x_{central}[1]^T; x_{central}[3]^T]^T$, and $x\\_j$ is obtained by $[x_{neighbor}[2]^T; x_{neighbor}[0]^T]^T$\n",
        "\n",
        "<font color='red'>For the following questions, DON'T refer to any existing implementations online.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RwG4HqCFCaOD"
      },
      "outputs": [],
      "source": [
        "class GraphSage(MessagePassing):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, normalize = True,\n",
        "                 bias = False, **kwargs):  \n",
        "        super(GraphSage, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message and update functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embedding \n",
        "        #            for central node.\n",
        "        # self.lin_r is the linear transformation that you apply to aggregated \n",
        "        #            message from neighbors.\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "        self.lin_l = torch.nn.Linear(self.in_channels, self.out_channels)\n",
        "        self.lin_r = torch.nn.Linear(self.in_channels, self.out_channels)\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        self.lin_r.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any post-processing (our update rule).\n",
        "        # 1. First call propagate function to conduct the message passing.\n",
        "        #    1.1 See there for more information: \n",
        "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        #    1.2 We use the same representations for central (x_central) and \n",
        "        #        neighbor (x_neighbor) nodes, which means you'll pass x=(x, x) \n",
        "        #        to propagate.\n",
        "        # 2. Update our node embedding with skip connection.\n",
        "        # 3. If normalize is set, do L-2 normalization (defined in \n",
        "        #    torch.nn.functional)\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "        out = self.propagate(edge_index, x=x, size=size)\n",
        "        out = self.lin_r(out)\n",
        "        x = self.lin_l(x)\n",
        "        out = x + out\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function here.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        out = x_j\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        # The axis along which to index number of nodes.\n",
        "        node_dim = self.node_dim\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: \n",
        "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='mean')\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1qx1bA2m1SWA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcfF3RACdLD"
      },
      "source": [
        "## GAT Implementation\n",
        "\n",
        "Attention mechanisms have become the state-of-the-art in many sequence-based tasks such as machine translation and learning sentence representations. One of the major benefits of attention-based mechanisms is their ability to focus on the most relevant parts of the input to make decisions. In this problem, we will see how attention mechanisms can be used to perform node classification of graph-structured data through the usage of Graph Attention Networks (GATs).\n",
        "\n",
        "The building block of the Graph Attention Network is the graph attention layer, which is a variant of the aggregation function . Let $N$ be the number of nodes and $F$ be the dimension of the feature vector for each node. The input to each graph attentional layer is a set of node features: $\\mathbf{h} = \\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}$\\}, $\\overrightarrow{h_i} \\in R^F$. The output of each graph attentional layer is a new set of node features, which may have a new dimension $F'$: $\\mathbf{h'} = \\{\\overrightarrow{h_1'}, \\overrightarrow{h_2'}, \\dots, \\overrightarrow{h_N'}\\}$, with $\\overrightarrow{h_i'} \\in \\mathbb{R}^{F'}$.\n",
        "\n",
        "We will now describe this transformation of the input features into higher-level features performed by each graph attention layer. First, a shared linear transformation parametrized by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ is applied to every node. Next, we perform self-attention on the nodes. We use a shared attentional mechanism:\n",
        "\\begin{equation} \n",
        "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
        "\\end{equation}\n",
        "\n",
        "This mechanism computes the attention coefficients that capture the importance of node $j$'s features to node $i$:\n",
        "\\begin{equation}\n",
        "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
        "\\end{equation}\n",
        "The most general formulation of self-attention allows every node to attend to all other nodes which drops all structural information. To utilize graph structure in the attention mechanisms, we can use masked attention. In masked attention, we only compute $e_{ij}$ for nodes $j \\in \\mathcal{N}_i$ where $\\mathcal{N}_i$ is some neighborhood of node $i$ in the graph.\n",
        "\n",
        "To easily compare coefficients across different nodes, we normalize the coefficients across $j$ using a softmax function:\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "\\end{equation}\n",
        "\n",
        "For this problem, our attention mechanism $a$ will be a single-layer feedforward neural network parametrized by a weight vector $\\overrightarrow{a} \\in \\mathbb{R}^{F'}$, followed by a LeakyReLU nonlinearity (with negative input slope 0.2). Let $\\cdot^T$ represent transposition and $||$ represent concatenation. The coefficients computed by our attention mechanism may be expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
        "\\end{equation}\n",
        "\n",
        "For the following questions, we denote $\\alpha_l = [...,\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i},...]$ and $\\alpha_r = [..., \\overrightarrow{a_r}^T \\mathbf{W_r} \\overrightarrow{h_j}, ...]$.\n",
        "\n",
        "\n",
        "At every layer of GAT, after the attention coefficients are computed for that layer, the aggregation function can be computed by a weighted sum of neighborhood messages, where weights are specified by $\\alpha_{ij}$.\n",
        "\n",
        "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
        "\n",
        "\\begin{equation}\n",
        "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
        "\\end{equation}\n",
        "\n",
        "To stabilize the learning process of self-attention, we use multi-head attention. To do this we use $K$ independent attention mechanisms, or ``heads'' compute output features as in the above equations. Then, we concatenate these output feature representations:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $||$ is concentation, $\\alpha_{ij}^{(k)}$ are the normalized attention coefficients computed by the $k$-th attention mechanism $(a^k)$, and $\\mathbf{W}^{(k)}$ is the corresponding input linear transformation's weight matrix. Note that for this setting, $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w4j45gTpCeXO"
      },
      "outputs": [],
      "source": [
        "class GAT(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, heads = 2,\n",
        "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "        self.att_l = None\n",
        "        self.att_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embeddings \n",
        "        # BEFORE message passing.\n",
        "        # Pay attention to dimensions of the linear layers, since we're using \n",
        "        # multi-head attention.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        self.lin_l = torch.nn.Linear(in_channels, heads * out_channels)\n",
        "        ############################################################################\n",
        "\n",
        "        self.lin_r = self.lin_l\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
        "        # You have to deal with multi-head scenarios.\n",
        "        # Use nn.Parameter instead of nn.Linear\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "        self.att_l = torch.nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
        "        self.att_r = torch.nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
        "        # 1. First apply linear transformation to node embeddings, and split that \n",
        "        #    into multiple heads. We use the same representations for source and\n",
        "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
        "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
        "        # 3. Call propagate function to conduct the message passing. \n",
        "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
        "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        # 4. Transform the output back to the shape of N * d.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        x_l = self.lin_l(x).reshape(-1, H, C)\n",
        "        x_r = self.lin_r(x).reshape(-1, H, C)\n",
        "        alpha_l = (self.att_l * x_l).sum(dim=-1)\n",
        "        alpha_r = (self.att_r * x_r).sum(dim=-1)\n",
        "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)\n",
        "        out = out.reshape(-1, H*C)\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function. Putting the attention in message \n",
        "        # instead of in update is a little tricky.\n",
        "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
        "        #    and apply leaky Relu.\n",
        "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
        "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
        "        # 3. Apply dropout to attention weights (alpha).\n",
        "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
        "        #    should be of shape E * H * d.\n",
        "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
        "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
        "        # Don't worry if you deviate from this.\n",
        "\n",
        "        alpha = F.leaky_relu(alpha_i + alpha_j, negative_slope=self.negative_slope)\n",
        "        if ptr:\n",
        "            att_weight = F.softmax(alpha, ptr)\n",
        "        else:\n",
        "            att_weight = torch_geometric.utils.softmax(alpha, index)\n",
        "        # Fill below with 2 lines \n",
        "        alpha = F.dropout(alpha, p=self.dropout)\n",
        "        out = alpha.unsqueeze(-1) * x_j\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
        "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        \n",
        "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='sum')\n",
        "        ############################################################################\n",
        "    \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2dkgSuWCheU"
      },
      "source": [
        "## Building Optimizers\n",
        "\n",
        "This function has been implemented for you. **For grading purposes please use the default Adam optimizer**, but feel free to play with other types of optimizers on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f_TIQ8NPCjBP"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYdWFwYCkwY"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Here we provide you with the functions to train and test. **Please do not modify this part for grading purposes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_tZMWRc8CmGg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train(dataset, args):\n",
        "    \n",
        "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
        "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, \n",
        "                            args)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    losses = []\n",
        "    test_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            pred = pred[batch.train_mask]\n",
        "            label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          test_acc = test(test_loader, model)\n",
        "          test_accs.append(test_acc)\n",
        "          print(\"Epoch \", epoch, \"Loss: \", total_loss, \"Test Acc.: \", test_acc)\n",
        "        else:\n",
        "          test_accs.append(test_accs[-1])\n",
        "    return test_accs, losses\n",
        "\n",
        "def test(loader, model, is_validation=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        mask = data.val_mask if is_validation else data.test_mask\n",
        "        # node classification: only evaluate on nodes in test set\n",
        "        pred = pred[mask]\n",
        "        label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    total = 0\n",
        "    for data in loader.dataset:\n",
        "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "    return correct / total\n",
        "  \n",
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-h7jIsCns4"
      },
      "source": [
        "## Let's Start the Training!\n",
        "\n",
        "We will be working on the CORA dataset on node-level classification.\n",
        "\n",
        "This part is implemented for you. **For grading purposes, please do not modify the default parameters.** However, feel free to play with different configurations just for fun!\n",
        "\n",
        "**Submit your best accuracy and loss on Gradescope.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qe9B45l9Cpz2",
        "outputId": "4a90acd6-6672-474a-eeb1-114b3ffd889b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node task. test set size: 140\n",
            "Epoch  0 Loss:  1.964883804321289 Test Acc.:  0.118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 Loss:  1.2721179723739624 Test Acc.:  0.418\n",
            "Epoch  20 Loss:  0.557070255279541 Test Acc.:  0.618\n",
            "Epoch  30 Loss:  0.2530808448791504 Test Acc.:  0.646\n",
            "Epoch  40 Loss:  0.20870107412338257 Test Acc.:  0.714\n",
            "Epoch  50 Loss:  0.16841277480125427 Test Acc.:  0.694\n",
            "Epoch  60 Loss:  0.11105477809906006 Test Acc.:  0.74\n",
            "Epoch  70 Loss:  0.13178984820842743 Test Acc.:  0.698\n",
            "Epoch  80 Loss:  0.13903608918190002 Test Acc.:  0.716\n",
            "Epoch  90 Loss:  0.10604298114776611 Test Acc.:  0.688\n",
            "Epoch  100 Loss:  0.10751408338546753 Test Acc.:  0.722\n",
            "Epoch  110 Loss:  0.11644040793180466 Test Acc.:  0.73\n",
            "Epoch  120 Loss:  0.11698609590530396 Test Acc.:  0.71\n",
            "Epoch  130 Loss:  0.09888893365859985 Test Acc.:  0.692\n",
            "Epoch  140 Loss:  0.13118775188922882 Test Acc.:  0.736\n",
            "Epoch  150 Loss:  0.1140202209353447 Test Acc.:  0.694\n",
            "Epoch  160 Loss:  0.08321963250637054 Test Acc.:  0.714\n",
            "Epoch  170 Loss:  0.13773579895496368 Test Acc.:  0.722\n",
            "Epoch  180 Loss:  0.07130332291126251 Test Acc.:  0.682\n",
            "Epoch  190 Loss:  0.0826781690120697 Test Acc.:  0.702\n",
            "Epoch  200 Loss:  0.12258046120405197 Test Acc.:  0.704\n",
            "Epoch  210 Loss:  0.11001626402139664 Test Acc.:  0.724\n",
            "Epoch  220 Loss:  0.15045370161533356 Test Acc.:  0.702\n",
            "Epoch  230 Loss:  0.10382583737373352 Test Acc.:  0.686\n",
            "Epoch  240 Loss:  0.11521130800247192 Test Acc.:  0.696\n",
            "Epoch  250 Loss:  0.07319305837154388 Test Acc.:  0.732\n",
            "Epoch  260 Loss:  0.08452197164297104 Test Acc.:  0.708\n",
            "Epoch  270 Loss:  0.12402879446744919 Test Acc.:  0.722\n",
            "Epoch  280 Loss:  0.09620686620473862 Test Acc.:  0.722\n",
            "Epoch  290 Loss:  0.0708770900964737 Test Acc.:  0.712\n",
            "Epoch  300 Loss:  0.08289989829063416 Test Acc.:  0.698\n",
            "Epoch  310 Loss:  0.0828934758901596 Test Acc.:  0.7\n",
            "Epoch  320 Loss:  0.0749087855219841 Test Acc.:  0.708\n",
            "Epoch  330 Loss:  0.20852719247341156 Test Acc.:  0.708\n",
            "Epoch  340 Loss:  0.07653344422578812 Test Acc.:  0.716\n",
            "Epoch  350 Loss:  0.07718618214130402 Test Acc.:  0.672\n",
            "Epoch  360 Loss:  0.12255634367465973 Test Acc.:  0.7\n",
            "Epoch  370 Loss:  0.12888063490390778 Test Acc.:  0.686\n",
            "Epoch  380 Loss:  0.0909673348069191 Test Acc.:  0.69\n",
            "Epoch  390 Loss:  0.08603768795728683 Test Acc.:  0.68\n",
            "Epoch  400 Loss:  0.1191030889749527 Test Acc.:  0.73\n",
            "Epoch  410 Loss:  0.096712626516819 Test Acc.:  0.698\n",
            "Epoch  420 Loss:  0.08239686489105225 Test Acc.:  0.708\n",
            "Epoch  430 Loss:  0.09351842105388641 Test Acc.:  0.716\n",
            "Epoch  440 Loss:  0.07552880048751831 Test Acc.:  0.686\n",
            "Epoch  450 Loss:  0.11163713783025742 Test Acc.:  0.718\n",
            "Epoch  460 Loss:  0.09611096978187561 Test Acc.:  0.708\n",
            "Epoch  470 Loss:  0.13710685074329376 Test Acc.:  0.708\n",
            "Epoch  480 Loss:  0.09101696312427521 Test Acc.:  0.728\n",
            "Epoch  490 Loss:  0.14814361929893494 Test Acc.:  0.726\n",
            "Maximum accuracy: 0.74\n",
            "Minimum loss: 0.041710175573825836\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# GraphSage\n",
        "\n",
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
        "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, \n",
        "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
        "         'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GraphSage']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\") \n",
        "            test_accs, losses = train(dataset, args) \n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1bYmz-q93HLk",
        "outputId": "c2663bef-29fc-4317-f57e-d16087992105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node task. test set size: 140\n",
            "Epoch  0 Loss:  1.9513710737228394 Test Acc.:  0.18\n",
            "Epoch  10 Loss:  1.9186680316925049 Test Acc.:  0.256\n",
            "Epoch  20 Loss:  2.0727381706237793 Test Acc.:  0.178\n",
            "Epoch  30 Loss:  1.808085322380066 Test Acc.:  0.19\n",
            "Epoch  40 Loss:  1.7955163717269897 Test Acc.:  0.194\n",
            "Epoch  50 Loss:  1.409307837486267 Test Acc.:  0.31\n",
            "Epoch  60 Loss:  1.3435412645339966 Test Acc.:  0.292\n",
            "Epoch  70 Loss:  1.231185793876648 Test Acc.:  0.342\n",
            "Epoch  80 Loss:  1.167215347290039 Test Acc.:  0.342\n",
            "Epoch  90 Loss:  1.1195265054702759 Test Acc.:  0.362\n",
            "Epoch  100 Loss:  1.0719034671783447 Test Acc.:  0.366\n",
            "Epoch  110 Loss:  1.0115538835525513 Test Acc.:  0.382\n",
            "Epoch  120 Loss:  0.978335440158844 Test Acc.:  0.364\n",
            "Epoch  130 Loss:  0.9922043681144714 Test Acc.:  0.39\n",
            "Epoch  140 Loss:  4.713088512420654 Test Acc.:  0.254\n",
            "Epoch  150 Loss:  49.1595458984375 Test Acc.:  0.156\n",
            "Epoch  160 Loss:  24.215057373046875 Test Acc.:  0.196\n",
            "Epoch  170 Loss:  54.756614685058594 Test Acc.:  0.258\n",
            "Epoch  180 Loss:  48.17017364501953 Test Acc.:  0.17\n",
            "Epoch  190 Loss:  209.7185516357422 Test Acc.:  0.194\n",
            "Epoch  200 Loss:  198.32781982421875 Test Acc.:  0.174\n",
            "Epoch  210 Loss:  99.71903991699219 Test Acc.:  0.26\n",
            "Epoch  220 Loss:  9.59525203704834 Test Acc.:  0.208\n",
            "Epoch  230 Loss:  8.659042358398438 Test Acc.:  0.184\n",
            "Epoch  240 Loss:  24.28670883178711 Test Acc.:  0.162\n",
            "Epoch  250 Loss:  3.3483169078826904 Test Acc.:  0.21\n",
            "Epoch  260 Loss:  3.4199018478393555 Test Acc.:  0.184\n",
            "Epoch  270 Loss:  1.9143415689468384 Test Acc.:  0.206\n",
            "Epoch  280 Loss:  2.264801263809204 Test Acc.:  0.212\n",
            "Epoch  290 Loss:  1.9875961542129517 Test Acc.:  0.204\n",
            "Epoch  300 Loss:  1.9112956523895264 Test Acc.:  0.216\n",
            "Epoch  310 Loss:  1.861935019493103 Test Acc.:  0.198\n",
            "Epoch  320 Loss:  1.7673498392105103 Test Acc.:  0.19\n",
            "Epoch  330 Loss:  1.8456178903579712 Test Acc.:  0.212\n",
            "Epoch  340 Loss:  1.8599724769592285 Test Acc.:  0.222\n",
            "Epoch  350 Loss:  2.1717453002929688 Test Acc.:  0.226\n",
            "Epoch  360 Loss:  1.7573133707046509 Test Acc.:  0.226\n",
            "Epoch  370 Loss:  1.687199592590332 Test Acc.:  0.24\n",
            "Epoch  380 Loss:  1.6904520988464355 Test Acc.:  0.226\n",
            "Epoch  390 Loss:  1.7656621932983398 Test Acc.:  0.198\n",
            "Epoch  400 Loss:  1.671669602394104 Test Acc.:  0.228\n",
            "Epoch  410 Loss:  1.7889156341552734 Test Acc.:  0.254\n",
            "Epoch  420 Loss:  1.6724529266357422 Test Acc.:  0.214\n",
            "Epoch  430 Loss:  1.7107923030853271 Test Acc.:  0.22\n",
            "Epoch  440 Loss:  1.6162893772125244 Test Acc.:  0.236\n",
            "Epoch  450 Loss:  1.6072049140930176 Test Acc.:  0.212\n",
            "Epoch  460 Loss:  1.8062800168991089 Test Acc.:  0.232\n",
            "Epoch  470 Loss:  1.6028417348861694 Test Acc.:  0.22\n",
            "Epoch  480 Loss:  1.6403321027755737 Test Acc.:  0.244\n",
            "Epoch  490 Loss:  1.631906509399414 Test Acc.:  0.216\n",
            "Maximum accuracy: 0.39\n",
            "Minimum loss: 0.8404027223587036\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# GAT\n",
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
        "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, \n",
        "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
        "         'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GAT']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\") \n",
        "            test_accs, losses = train(dataset, args) \n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHELqjARZ1W5"
      },
      "source": [
        "## Question 1.1: What is the maximum accuracy you could get on test set for GraphSage?\n",
        "\n",
        "Maximum accuracy: 0.736"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlCtBEBLMBkR"
      },
      "source": [
        "## Question 1.2: What is the maximum accuracy you could get on test set for GAT?\n",
        "\n",
        "Maximum accuracy: 0.382"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "In order to get credit, you need to submit the `ipynb` file of Colab 3 to LMS.\n",
        "To get this file, click `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}